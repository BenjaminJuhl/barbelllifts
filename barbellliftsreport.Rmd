---
title: "Classifying barbell lifts from accelerometer measurements"
author: "Benjamin Juhl"
date: "20. April 2016"
output: html_document
---
```{r include=FALSE} 
require(dplyr)
require(caret)
require(randomForest)
``` 

# Synopsis
The aim of this project is to predict barbell lift classes from accelerometers 
measurements. An exploratory analysis finds no strong correlations with any 
predictors, so two out of the box classifier methods are used to create models 
and validate them - random forests (rf) and boosting with trees (gbm). The 
estimated out of sample accuracy is better for the Random Forest model, and 
thus, this model will be used for further predictions.
# Data processing
The data used for the prediction model was provided by the Course team and 
originates from 'Groupware@LES' <http://groupware.les.inf.puc-rio.br/har>. The 
training data was downloaded from 
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>.
In order to predict classes in relation to accelerometer measurements, first 
variables not connected to these measurements are removed, second, variables 
with missing values are removed, and last, variables with near zero variance 
are removed from the data set.
.
```{r echo=TRUE,cache=TRUE}
set.seed(12703)
## read the data
data <- read.csv("pml-training.csv",na.strings=c("NA","","#DIV/0!"))
data <- tbl_df(data)
testing <- read.csv("pml-testing.csv",na.strings=c("NA","","#DIV/0!"))
testing <- tbl_df(testing)
## remove the non accelorator variables
data <- data[,-(1:7)]
testing <- testing[,-(1:7)]
## remove columns with missing values
testing <- testing[,colSums(is.na(data))==0]
data <- data[,colSums(is.na(data))==0]
## remove near zero variance variables
nearzero <- nearZeroVar(data,saveMetrics = TRUE)
testing <- testing[,!nearzero$nzv]
data <- data[,!nearzero$nzv]
## split into training and validation sets
indata <- createDataPartition(y=data$classe,p=0.75,list=FALSE)
validating <- data[-indata,]
data <- data[indata,]
dim(data)
```
After these processing steps, the data set contains 19622 observations for the 
outcome (classe) and 52 predictors.

# Exploratory data analysis
As a first approach, direction uncoupled variables are examined for a pattern 
related to classe.
```{r, echo=TRUE,fig.cap="Pairs plot of the direction uncoupled variables variables",cache=TRUE}
gennames <- c("roll_belt","pitch_belt","yaw_belt","roll_arm","pitch_arm",
              "yaw_arm","roll_dumbbell","pitch_dumbbell","yaw_dumbbell",
              "roll_forearm","pitch_forearm","yaw_forearm")
gen_data <- data[,gennames]
gen_data <- cbind(data$classe,gen_data)
gen_data <- tbl_df(gen_data)
pairs(gen_data,col=gen_data$`data$classe`)
```
No clear pattern related to classe seems visible.

## Singular value decomposition
Examining if a singular value decomposition can separate the classes better:
```{r, echo=TRUE,fig.cap="The First three left singular vextors colored by classe",cache=TRUE}
sdata <- data[,-53]
scdata <- scale(sdata)
svd1 <- svd(scdata)
par(mfrow=c(1,3))
plot(svd1$u[,1],col=data$classe)
plot(svd1$u[,2],col=data$classe)
plot(svd1$u[,3],col=data$classe)
```
The classes do not seem to be separated any better than before the singular value 
decomposition.

# Prediction model
As there was no clear correlation for any of the predictors or the left singular 
vectors, any linear style model does not seem promising. Therefore a random forest 
and a gbm approach is used to train the model.
## GBM model
```{r, echo=TRUE,cache=TRUE}
modelgbm <- train(classe~.,data=data,preProcess=c("center","scale"),method="gbm")
predgbm_train <- predict(modelgbm)
gbminsample <- confusionMatrix(predgbm_train,data$classe)$overall[1]
```
The in sample accuracy for the gbm model is `r gbminsample`.

## Random Forest model
```{r, echo=TRUE,cache=TRUE}
modelrf <- train(classe~.,data=data,preProcess=c("center","scale"),method="rf",importance = T, trControl = trainControl(method = "cv", number = 4))
predrf_train <- predict(modelrf)
rfinsample <- confusionMatrix(predrf_train,data$classe)$overall[1]
```
The in sample accuracy for the rf model is `r rfinsample`.

# Cross validation
## GBM model
```{r, echo=TRUE,cache=TRUE}
predgbm_val <- predict(modelgbm,newdata = validating)
gbmoutofsample <- confusionMatrix(predgbm_val,validating$classe)$overall[1]
confusionMatrix(predgbm_val,validating$classe)
```
The estimated out of sample error for the rf model is `r gbmoutofsample`.

## Random forest model
```{r, echo=TRUE,cache=TRUE}
predrf_val <- predict(modelrf,newdata = validating)
rfoutofsample <- confusionMatrix(predrf_val,validating$classe)$overall[1]
confusionMatrix(predrf_val,validating$classe)
```
The estimated out of sample error for the rf model is `r rfoutofsample`.

# Prediction
The Random Forest model has a better estimated out of sample accuracy, 
and will therefore be used for further predictions on the provided test 
set.
```{r, echo=TRUE,cache=TRUE}
dim(testing)
predict(modelrf,newdata=testing)
```


